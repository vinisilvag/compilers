README file for Programming Assignment 2 (C++ edition)
=====================================================

Your directory should now contain the following files:

 Makefile        -> [course dir]/src/PA2/Makefile
 README
 cool.flex
 test.cl
 lextest.cc      -> [course dir]/src/PA2/lextest.cc
 mycoolc         -> [course dir]/src/PA2/mycoolc
 stringtab.cc    -> [course dir]/src/PA2/stringtab.cc
 utilities.cc    -> [course dir]/src/PA2/utilities.cc
 handle_flags.cc -> [course dir]/src/PA2/handle_flags.cc
 *.d             dependency files
 *.*             other generated files

The include (.h) files for this assignment can be found in 
[course dir]/include/PA2

	The Makefile contains targets for compiling and running your
	program. DO NOT MODIFY.

	The README contains this info. Part of the assignment is to fill
	the README with the write-up for your project. You should
	explain design decisions, explain why your code is correct, and
	why your test cases are adequate. It is part of the assignment
	to clearly and concisely explain things in text as well as to
	comment your code. Just edit this file.

	cool.flex is a skeleton file for the specification of the
	lexical analyzer. You should complete it with your regular
	expressions, patterns and actions. Information on how to do this
	is in the flex manual, which is part of your reader.

	test.cl is a COOL program that you can test the lexical
	analyzer on. It contains some errors, so it won't compile with
	coolc. However, test.cl does not exercise all lexical
	constructs of COOL and part of your assignment is to rewrite
	test.cl with a complete set of tests for your lexical analyzer.

	cool-parse.h contains definitions that are used by almost all parts
	of the compiler. DO NOT MODIFY.

	stringtab.{cc|h} and stringtab_functions.h contains functions
        to manipulate the string tables.  DO NOT MODIFY.

	utilities.{cc|h} contains functions used by the main() part of
	the lextest program. You may want to use the strdup() function
	defined in here. Remember that you should not print anything
	from inside cool.flex! DO NOT MODIFY.

	lextest.cc contains the main function which will call your
	lexer and print out the tokens that it returns.  DO NOT MODIFY.

	mycoolc is a shell script that glues together the phases of the
	compiler using Unix pipes instead of statically linking code.  
	While inefficient, this architecture makes it easy to mix and match
	the components you write with those of the course compiler.
	DO NOT MODIFY.	

        cool-lexer.cc is the scanner generated by flex from cool.flex.
        DO NOT MODIFY IT, as your changes will be overritten the next
        time you run flex.

 	The *.d files are automatically generated Makefiles that capture
 	dependencies between source and header files in this directory.
 	These files are updated automatically by Makefile; see the gmake
 	documentation for a detailed explanation.

Instructions
------------

	To compile your lextest program type:

	% make lexer

	Run your lexer by putting your test input in a file 'foo.cl' and
	run the lextest program:

	% ./lexer foo.cl

	To run your lexer on the file test.cl type:

	% make dotest

	If you think your lexical analyzer is correct and behaves like
	the one we wrote, you can actually try 'mycoolc' and see whether
	it runs and produces correct code for any examples.
	If your lexical analyzer behaves in an
	unexpected manner, you may get errors anywhere, i.e. during
	parsing, during semantic analysis, during code generation or
	only when you run the produced code on spim. So beware.

	To turnin your work type:

	% make submit-clean

	After that,  collect the files cool.flex, test.cl,
	README, and test.output into a .tar.gz or .zip file and submit
	in our moodle page. Don't forget to edit the README file to
	include your write-up, and to write your own test cases in
	test.cl.

 	You may turn in the assignment as many times as you like.
	However, only the last version will be retained for
	grading.

	GOOD LUCK!

---8<------8<------8<------8<---cut here---8<------8<------8<------8<---

Write-up for PA2
----------------

Trabalho realizado por Vinicius Silva Gomes - 2021421869 para a disciplina
DCC053 - Compiladores I, no semestre 2023/1.



Para realizar a análise léxica de programas bem (ou mal) formados da
linguagem de programação COOL (Classroom Object-Oriented Language),
foi usado o Flex, que é um gerador de analisadores léxicos para linguagens
de programação. O Flex fornece uma API onde podemos especificar os
tokens de linguagens de programação utilizando, dentre outras construções
mais específicas da ferramenta, expressões e definições regulares.

Dessa forma, a seção a seguir irá tratar sobre a abordagem utilizada para
realizar a conversão de programas COOL para a sequência de tokens que define
o mesmo programa, utilizando a estrutura e o ferramental fornecido pelo Flex.



Análisador Léxico para a linguagem COOL

Váriaveis auxiliares

Primeiramente, foram declaradas três variáveis auxiliares que ajudarão na
leitura de comentários e strings da linguagem. As variáveis são:

* commentLevel: de tipo inteiro, essa variável é responsável por indicar
se existe um bloco de comentário aberto ou não;

* readString: de tipo string, essa variável é responsável por armazenar
o texto lido por uma string da linguagem COOL. O uso dela é interessante
pois facilita a manipulação do caractere \ nas construções da linguagem
(quebra de linha dentro da string, por exemplo);

* readNullToken: de tipo inteiro, essa variável é responsável por indicar
se a string que está sendo lida contém o caractere nulo (\0). Essa flag
será usada posteriormente para testar se o analisador léxico deve acusar
erro ou não, caso a o caractere nulo tenha sido lido ao longo da string.

A medida que se fizerem necessárias, o comportamento e motivação de cada
uma das variáveis será explicado e seu uso e necessidade ficará mais
evidente.



Definições Regulares

Para facilitar o matching e a tokenização das estruturas da linguagem, 
algumas definições regulares foram escritas, de modo que a definição das
regras de pattern matching no Flex fosse facilitada.

Apesar de não ser necessário, como as palavras reservadas de COOL são
case-insensitive, todas elas foram escritas atráves de expressões
regulares para facilitar a representação na seção de regras. As 
palavras-chave class e while foram, por exemplo, representadas através das
expressões regulares (?i:class) e (?i:while), respectivamente.

As únicas exceções para as palavras reservadas da linguagem eram os operadores
booleanos true e false, que devem começar com letra minúscula e, a partir
daí, são case-insensitive. Dessa forma, a expressão regular para representar
esses operadores foi t(?i:rue) e f(?i:alse).

Além das palavras reservadas, outras definições regulares foram escritas.
Dentre essas definições, estão presentes àquelas usadas para a representação de
espaços em branco, caracteres alfanuméricos e a representação de
identificadores de tipo e objetos (variáveis, funções, etc).

Por fim, três padrões para a ativação de regras de maneira condicional foram
definidos. Eles são: MULTILINE_COMMENT, SINGLELINE_COMMENT e STRING. Como os
nomes sugerem, eles serão responsáveis por auxiliar na leitura e 
tratamento de comentários e strings no analisador léxico. Mais detalhes
a forma como cada um foi usado serão providos na próxima seção.



Regras

As primeiras regras especificadas são àquelas relativas a leitura e
tratamento de comentários e strings. Para ambas as construções, o uso de 
"start conditions" foi extremamente necessário para que todas as tratativas
possam ser feitas.

Começando pelos comentários, a ideia é que sempre que a subcadeia '(*' seja encontrada,
um comentário é incializado, ó que é indicado pelo incremento da variável commentLevel
e o início da regra MULTILINE_COMMENT, definida previamente. Nessa regra, todos os
caracteres lidos são consumidos sem que nenhuma ação seja tomada, com exceção da subcadeia
'*)', que indica que um fim de comentário foi encontrado e, portanto, commentLevel deve ser
decrementada. Caso commentLevel chegue a zero, saímos da regra MULTILINE_COMMENT e voltamos
para o "contexto" comum de execução.

Além disso, caso EOF (fim de arquivo) seja encontrado, o erro "EOF in comment" é retornado
e caso '*)' seja encontrado fora do contexto de comentário ou string, ou seja, no contexto
normal de execução, o erro "Unmatched *)" é retornado.

Por fim, caso a subcadeia '--' seja encontrada, a regra SINGLELINE_COMMENT é ativada. Nessa
regra, todos os caracteres, com exceção do \n, são consumidos sem executar nenhuma ação. Caso
um \n seja lido, a variável do próprio Flex curr_lineno, responsável por armazenar
o número de linhas lidos até o momento, é incrementada.

Para as strings, a estratégia é relativamente semelhante. Se estamos na regra inicial e
encontramos o caractere '"' as variáveis auxiliares para a leitura de strings são resetadas
e iniciamos uma regra STRING.

Nessa regra, os caracteres vão sendo lidos do programa e acumulados na variável readString
até que algum caractere começado com \ seja encontrado. A regra que trata esse tipo de
ocorrência garante que quebras de linha propositalmente escritos sejam devidamente lidas, que
quebras de linha do programa sejam sempre precedidas por \, as outras subcadeias da linguagem
começadas com \ e o caractere nulo \0. Caso esse último seja encontrado, a flag readNullToken
será marcada como 1, indicando que a string é inválida.

Caso um \n que não seja precedido por um \ seja lido, o erro "Unterminated string constant"
será imediatamente disparado.

Caso o caractere '"' seja encontrado novamente, mas sob o contexto da regra STRING,
significa que ela terminou de ser lida e mais erros podem ser encontrados/tratados. No caso,
o contexto retorna para o contexto inicial e as condições de validade que devem ser
avaliadas ao final da leitura da string serão testadas. Se a flag readNullToken for 1, o
erro "String contains null character" é retornado e se a string lida tiver 1025 ou mais caracteres,
o erro "String constant too long" é disparado. Por fim, se nenhum erro foi retornado,
a string lida é adicionada à tabela de símbolos de strings.

O último caso a ser avaliado é aquele onde o EOF é encontrado dentro do contexto de STRING.
Nessa situação, o erro "EOF in string" será retornado.

Uma vez estabelecido a forma de lidar com as construções mais complexas e problemáticas,
do ponto de vista sintático, para a linguagem COOL, as outras regras para
tradução de tokens podem ser especificadas.

A tokenização dos operadores, caracteres de pontuação, etc, da linguagem é
bastante simples. Para as construções que são escritas com apenas um caractere,
como '+', '-', '*', '(', ')', '{', '}', '.', ';', etc; basta retornar o lexema
lido como próprio token. Para as construções que usam dois (ou mais, caso existissem)
caracteres, como '<-' e '<=', por exemplo, o token retornado é um texto que identifique
facilmente o lexema. No caso dos exemplos, os tokens retornados foram ASSIGN (referente
a  atribuição de valores em COOL) e LE (referente ao operador de comparação menor
ou igual), respectivamente.

Para as palavras reservadas, as regras são as mais simples possíveis: uma vez
que a definição regular de alguma palavra foi encontrada no texto,  a palavra reservada
correspondente em caixa alta será retornada como token reconhecido.

Temos ainda a tokenização das constantes booleanas, de literais inteiros e 
de identificadores, tanto de tipos quanto de objetos. Para todos os casos, a
regra definida é mais ou menos a mesma: a subcadeia lida é inserida na devida
tabela (de inteiros, id's ou valor booleano setado, caso seja um operador
boolean true ou false) e o token da construção é retornado, sendo ele BOOL_CONST,
INT_CONST, TYPEID ou OBJECTID, para valores booleanos, inteiros, identificadores
de tipos e identificadores de objetos, respecitivamente.

Caso um \n (quebra de linha) seja encontrado, a regra tomada é incrementar
a variável curr_lineno.

Caso algum espaço em branco, codificado pela definição regular WHITESPACE,
seja encontrado, ele apenas é consumido, sem que nada a mais seja feito.

Por fim, caso a subcadeia lida não se aplique a nenhuma das regras definidas,
ela irá cair numa última regra de erro, que retorna ERROR e copia a subcadeia
mal construída para o buffer de mensagem de erro.

----------------

Com todas essas construções descritas, é possível então realizar a análise
léxica de programas COOL, tanto os bem construídos quanto os mal construídos.
Além disso, caso algum programa esteja mal construído do ponto de vista léxico,
o analisador será capaz de identificar os erros e retornar para o usuário qual
o erro ou construção incorreta foi identificado e em qual linha ele foi
identificado.

Além disso, para exercitar o analisador léxico construído, alguns testes foram
desenvolvidos para verificar que ele é capaz de tokenizar corretamente os mais
diversos, complexos e completos programas COOL da maneira esperada, tokenizando
corretamente quando a construção é adequada, ou apresentando o devido erro,
quando ela não está bem formulada.

Assim como pedido, além dos testes construídos durante o desenvolvimento do
analisador, o programa test.cl foi completamente reescrito, se tornando um
novo e grande programa COOL que não apresenta erros e é capaz de ser analisado
corretamente pelo lexer construído, ser compilado pelo compilador de COOL e
executado pelo simulador do MIPS.
