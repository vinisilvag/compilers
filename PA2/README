README file for Programming Assignment 2 (C++ edition)
=====================================================

Your directory should now contain the following files:

 Makefile        -> [course dir]/src/PA2/Makefile
 README
 cool.flex
 test.cl
 lextest.cc      -> [course dir]/src/PA2/lextest.cc
 mycoolc         -> [course dir]/src/PA2/mycoolc
 stringtab.cc    -> [course dir]/src/PA2/stringtab.cc
 utilities.cc    -> [course dir]/src/PA2/utilities.cc
 handle_flags.cc -> [course dir]/src/PA2/handle_flags.cc
 *.d             dependency files
 *.*             other generated files

The include (.h) files for this assignment can be found in 
[course dir]/include/PA2

	The Makefile contains targets for compiling and running your
	program. DO NOT MODIFY.

	The README contains this info. Part of the assignment is to fill
	the README with the write-up for your project. You should
	explain design decisions, explain why your code is correct, and
	why your test cases are adequate. It is part of the assignment
	to clearly and concisely explain things in text as well as to
	comment your code. Just edit this file.

	cool.flex is a skeleton file for the specification of the
	lexical analyzer. You should complete it with your regular
	expressions, patterns and actions. Information on how to do this
	is in the flex manual, which is part of your reader.

	test.cl is a COOL program that you can test the lexical
	analyzer on. It contains some errors, so it won't compile with
	coolc. However, test.cl does not exercise all lexical
	constructs of COOL and part of your assignment is to rewrite
	test.cl with a complete set of tests for your lexical analyzer.

	cool-parse.h contains definitions that are used by almost all parts
	of the compiler. DO NOT MODIFY.

	stringtab.{cc|h} and stringtab_functions.h contains functions
        to manipulate the string tables.  DO NOT MODIFY.

	utilities.{cc|h} contains functions used by the main() part of
	the lextest program. You may want to use the strdup() function
	defined in here. Remember that you should not print anything
	from inside cool.flex! DO NOT MODIFY.

	lextest.cc contains the main function which will call your
	lexer and print out the tokens that it returns.  DO NOT MODIFY.

	mycoolc is a shell script that glues together the phases of the
	compiler using Unix pipes instead of statically linking code.  
	While inefficient, this architecture makes it easy to mix and match
	the components you write with those of the course compiler.
	DO NOT MODIFY.	

        cool-lexer.cc is the scanner generated by flex from cool.flex.
        DO NOT MODIFY IT, as your changes will be overritten the next
        time you run flex.

 	The *.d files are automatically generated Makefiles that capture
 	dependencies between source and header files in this directory.
 	These files are updated automatically by Makefile; see the gmake
 	documentation for a detailed explanation.

Instructions
------------

	To compile your lextest program type:

	% make lexer

	Run your lexer by putting your test input in a file 'foo.cl' and
	run the lextest program:

	% ./lexer foo.cl

	To run your lexer on the file test.cl type:

	% make dotest

	If you think your lexical analyzer is correct and behaves like
	the one we wrote, you can actually try 'mycoolc' and see whether
	it runs and produces correct code for any examples.
	If your lexical analyzer behaves in an
	unexpected manner, you may get errors anywhere, i.e. during
	parsing, during semantic analysis, during code generation or
	only when you run the produced code on spim. So beware.

	To turnin your work type:

	% make submit-clean

	After that,  collect the files cool.flex, test.cl,
	README, and test.output into a .tar.gz or .zip file and submit
	in our moodle page. Don't forget to edit the README file to
	include your write-up, and to write your own test cases in
	test.cl.

 	You may turn in the assignment as many times as you like.
	However, only the last version will be retained for
	grading.

	GOOD LUCK!

---8<------8<------8<------8<---cut here---8<------8<------8<------8<---

Write-up for PA2
----------------

Para realizar a análise léxica de programas bem (ou mal) formados da
linguagem de programação COOL (Classroom Object-Oriented Language),
foi usado o Flex, que é um gerador de analisadores léxicos para linguagens
de programação. O Flex fornece uma API onde podemos especificar os
tokens de linguagens de programação utilizando, dentre outras construções
mais específicas da ferramenta, expressões e definições regulares.

Dessa forma, a seção a seguir irá tratar sobre a abordagem utilizada para
realizar a conversão de programas COOL para a sequência de tokens que define
o mesmo programa, utilizando a estrutura e o ferramental fornecido pelo Flex.

Análisador Léxico para a linguagem COOL

Váriaveis auxiliares

Primeiramente, foram declaradas três variáveis auxiliares que ajudarão na
leitura de comentários e strings da linguagem. As variáveis são:

* commentLevel: de tipo inteiro, essa variável é responsável por indicar
se existe um bloco de comentário aberto ou não;

* readString: de tipo string, essa variável é responsável por armazenar
o texto lido por uma string da linguagem COOL. O uso dela é interessante
pois facilita a manipulação do caractere \ nas construções da linguagem
(quebra de linha dentro da string, por exemplo);

* readNullToken: de tipo inteiro, essa variável é responsável por indicar
se a string que está sendo lida contém o caractere nulo (\0). Essa flag
será usada posteriormente para testar se o analisador léxico deve acusar
erro ou não, caso a o caractere nulo tenha sido lido ao longo da string.

A medida que se fizerem necessárias, o comportamento e motivação de cada
uma das variáveis será explicado e seu uso e necessidade ficará mais
evidente.

Definições Regulares

Para facilitar o matching e a tokenização das estruturas da linguagem, 
algumas definições regulares foram escritas, de modo que a definição das
regras de pattern matching no Flex fosse facilitada.

Apesar de não ser necessário, como as palavras reservadas de COOL são
case-insensitive, todas elas foram escritas atráves de expressões
regulares para facilitar a representação na seção de regras. As 
palavras-chave class e while foram, por exemplo, representadas através das
expressões regulares (?i:class) e (?i:while), respectivamente.

As únicas exceções para as palavras reservadas da linguagem eram os operadores
booleanos true e false, que devem começar com letra minúscula e, a partir
daí, são case-insensitive. Dessa forma, a expressão regular para representar
esses operadores foi t(?i:rue) e f(?i:alse).

Além das palavras reservadas, outras definições regulares foram escritas.
Dentre essas definições, estão presentes àquelas usadas para a representação de
espaços em branco, caracteres alfanuméricos e a representação de
identificadores de tipo e objetos.

Por fim, três padrões para a ativação de regras de maneira condicional foram
definidos. Eles são: MULTILINE_COMMENT, SINGLELINE_COMMENT e STRING. Como os
nomes sugerem, eles serão responsáveis por auxiliar na leitura e 
tratamento de comentários e strings no analisador léxico. Mais detalhes
a forma como cada um foi usado serão providos na próxima seção.

Regras

As primeiras regras especificadas são àquelas relativas a leitura e
tratamento de comentários e strings. Para ambas as construções, o uso de 
"start conditions" foi extremamente necessário para que todas as tratativas
possam ser feitas.

Começando pelos comentários

Para as palavras reservadas, as regras são as mais simples possíveis: uma vez
que a definição regular de alguma palavra foi encontrada no texto,  a palavra reservada
correspondente em caixa alta será retornada como token reconhecido.

Caso um \n (quebra de linha) seja encontrado, a regra tomada é incrementar
a variável curr_lineno.

Caso algum espaço em branco, codificado pela definição regular WHITESPACE,
seja encontrado, ele apenas é consumido, sem que nada a mais seja feito.

Por fim, caso a subcadeia lida não se aplique a nenhuma das regras definidas,
ela irá cair numa última regra de erro, que retorna ERROR e copia a subcadeia
mal construída para o buffer de mensagem de erro.



Com todas essas construções descritas, é possível então realizar a análise
léxima de programas COOL, tanto os bem construídos quanto os mal construídos.
Além disso, caso algum programa esteja mal construído do ponto de vista léxico,
o analisador será capaz de identificar esse erro e retornar para o usuário qual
o erro ou construção incorreta foi identificado e em qual linha ele foi
identificado.

Além disso, para exercitar o analisador léxico construído, alguns testes foram
desenvolvidos para verificar que ele é capaz de tokenizar corretamente os mais
diversos, complexos e completos programas COOL da maneira esperada, tokenizando
corretamente quando a construção é adequada, ou apresentando o devido erro,
quando ela não está bem formulada.

Assim como pedido, além dos testes construídos durante o desenvolvimento do
analisador, o programa test.cl foi completamente reescrito, se tornando um
novo e grande programa COOL que não apresenta erros e é capaz de ser analisado
corretamente pelo lexer construído, ser compilado pelo compilador de COOL e
executado pelo simulador do MIPS.
